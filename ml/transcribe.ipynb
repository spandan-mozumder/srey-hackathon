{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(50548) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (20240930)\n",
      "Requirement already satisfied: numba in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from openai-whisper) (2.0.2)\n",
      "Requirement already satisfied: torch in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from openai-whisper) (2.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: more-itertools in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from openai-whisper) (10.6.0)\n",
      "Requirement already satisfied: tiktoken in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from openai-whisper) (0.9.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: filelock in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from torch->openai-whisper) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from torch->openai-whisper) (4.13.1)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from torch->openai-whisper) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from torch->openai-whisper) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from torch->openai-whisper) (2024.12.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from sympy!=1.13.2,>=1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Tried Whisper on apple silicon but doesn't play nice with Metal Performance Shader yet\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"small\", device=device) \n",
    "\n",
    "# using small instead of base for better accuracy\n",
    "# try medium or large for better accuracy but slower speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transcribe using whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rudrarajkundu/devdrive/pytorch/careloop-ml/lib/python3.9/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
      "python(50549) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:\n",
      "\n",
      " I had a great experience at the hospital. The staff was friendly and attentive.\n"
     ]
    }
   ],
   "source": [
    "AUDIO_PATH = \"data/sample_audio.mp3\"\n",
    "\n",
    "if os.path.exists(AUDIO_PATH):\n",
    "    print(\"Processing audio file...\")\n",
    "    result = model.transcribe(AUDIO_PATH)\n",
    "    print(\"Transcript:\\n\")\n",
    "    print(result[\"text\"])\n",
    "else:\n",
    "    print(\"‚ùå Audio file not found. Make sure it's at `data/sample_audio.mp3`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auto summarise using facebook/bart-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Your max_length is set to 150, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \"I had a great experience at the hospital. The staff was friendly and attentive,\" says patient. \"I was able to spend time with my family and friends,\" adds patient.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        summary = summarizer(text, max_length=150, min_length=30, do_sample=False)[0][\"summary_text\"] # Adjust max_length and min_length as needed\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing text: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "patient_feedback_text = result[\"text\"]  \n",
    "summary = summarize_text(patient_feedback_text)\n",
    "\n",
    "if summary:\n",
    "    print(f\"Summary: {summary}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Concern Tagging using Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Concern Tags:\n",
      "- Staff: 0.47\n",
      "- Communication: 0.11\n",
      "- Comfort and Privacy: 0.10\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "concern_categories = [\"Staff\", \"Billing\", \"Food and Amenities\", \"Cleanliness\", \"Post Discharge Care\", \"Communication\", \"Efficiency\", \"Comfort and Privacy\", \"Digital Experience\"]\n",
    "\n",
    "def tag_concerns_top_n(text, categories, top_n=3):\n",
    "    \"\"\"\n",
    "    Tags the input text with the top N most likely concern categories.\n",
    "\n",
    "    Args:\n",
    "        text (str): The patient feedback text.\n",
    "        categories (list): A list of potential concern categories.\n",
    "        top_n (int): The number of top categories to return (default is 3).\n",
    "\n",
    "    Returns:\n",
    "        list or None: A list of tuples, where each tuple contains (category, score),\n",
    "                     or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = classifier(text, candidate_labels=categories)\n",
    "        if result and result['labels'] and result['scores']:\n",
    "            top_results = list(zip(result['labels'][:top_n], result['scores'][:top_n]))\n",
    "            return top_results\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error tagging concern: {e}\")\n",
    "        return None\n",
    "\n",
    "# Your input text\n",
    "feedback_text = summary\n",
    "\n",
    "# Get the top 3 concern tags\n",
    "top_concerns = tag_concerns_top_n(feedback_text, concern_categories, top_n=3)\n",
    "\n",
    "if top_concerns:\n",
    "    print(f\"Top Concern Tags:\")\n",
    "    for tag, score in top_concerns:\n",
    "        print(f\"- {tag}: {score:.2f}\")\n",
    "else:\n",
    "    print(\"Could not determine concern tags.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis using nlptown/bert-base-multilingual-uncased-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive (Score: 0.44)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "def analyze_sentiment_simplified(text):\n",
    "    \"\"\"\n",
    "    Analyzes the sentiment of the input text and maps it to positive, negative, or neutral.\n",
    "\n",
    "    Args:\n",
    "        text (str): The patient feedback text.\n",
    "\n",
    "    Returns:\n",
    "        tuple or None: A tuple containing the simplified sentiment label ('positive', 'negative', 'neutral')\n",
    "                       and the confidence score, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = sentiment_analyzer(text)[0]\n",
    "        star_label = result['label']\n",
    "        score = result['score']\n",
    "\n",
    "        if star_label in ['4 stars', '5 stars']:\n",
    "            sentiment = 'positive'\n",
    "        elif star_label in ['1 star', '2 stars']:\n",
    "            sentiment = 'negative'\n",
    "        else:  # '3 stars'\n",
    "            sentiment = 'neutral'\n",
    "\n",
    "        return sentiment, score\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing sentiment: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Your input text\n",
    "feedback_text = summary\n",
    "sentiment_label, sentiment_score = analyze_sentiment_simplified(feedback_text)\n",
    "\n",
    "if sentiment_label:\n",
    "    print(f\"Sentiment: {sentiment_label} (Score: {sentiment_score:.2f})\")\n",
    "else:\n",
    "    print(\"Could not determine sentiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': ' I had a great experience at the hospital. The staff was friendly and attentive.', 'summary': '\"I had a great experience at the hospital. The staff was friendly and attentive,\" says patient. \"I was able to spend time with my family and friends,\" adds patient.', 'concern_tags': [{'tag': 'Staff', 'score': 0.4650678336620331}, {'tag': 'Communication', 'score': 0.10677339881658554}, {'tag': 'Comfort and Privacy', 'score': 0.10124950110912323}], 'sentiment': 'positive', 'sentiment_score': 0.43942055106163025}\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'results', 'summary', 'top_concerns', 'sentiment', and 'sentiment_score' are already defined\n",
    "\n",
    "processed_feedback = {\n",
    "    \"transcription\": result[\"text\"] if \"text\" in result else None,\n",
    "    \"summary\": summary,\n",
    "    \"concern_tags\": [],\n",
    "    \"sentiment\": sentiment_label,\n",
    "    \"sentiment_score\": sentiment_score\n",
    "}\n",
    "\n",
    "# Iterate through the list of top concerns and add them to the dictionary\n",
    "if isinstance(top_concerns, list):\n",
    "    for tag, score in top_concerns:\n",
    "        processed_feedback[\"concern_tags\"].append({\"tag\": tag, \"score\": score})\n",
    "\n",
    "print(processed_feedback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "careloop-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
